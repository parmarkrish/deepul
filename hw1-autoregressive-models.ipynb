{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/rll/deepul/blob/master/homeworks/hw1/hw1_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Getting Started\n\n## Overview\nThis semester, all homeworks will be conducted through Google Colab notebooks. All code for the homework assignment will be written and run in this notebook. Running in Colab will automatically provide a GPU, but you may also run this notebook locally by following [these instructions](https://research.google.com/colaboratory/local-runtimes.html) if you wish to use your own GPU.\n\nYou will save images in the notebooks to use and fill out a given LaTeX template which will be submitted to Gradescope, along with your notebook code.\n\n## Using Colab\nOn the left-hand side, you can click the different icons to see a Table of Contents of the assignment, as well as local files accessible through the notebook.\n\nMake sure to go to **Runtime -> Change runtime type** and select **GPU** as the hardware accelerator. This allows you to use a GPU. Run the cells below to get started on the assignment. Note that a session is open for a maximum of 12 hours, and using too much GPU compute may result in restricted access for a short period of time. Please start the homework early so you have ample time to work.\n\n**If you loaded this notebook from clicking \"Open in Colab\" from github, you will need to save it to your own Google Drive to keep your work.**\n\n## General Tips\nIn each homework problem, you will implement an autoregressive model and run it on two datasets (dataset 1 and dataset 2). The expected outputs for dataset 1 are already provided to help as a sanity check.\n\nFeel free to print whatever output (e.g. debugging code, training code, etc) you want, as the graded submission will be the submitted pdf with images.\n\nAfter you complete the assignment, download all of the image outputted in the results/ folder and upload them to the figure folder in the given latex template.\n\nRun the cells below to download and load up the starter code.","metadata":{"id":"Rdy1FtrRpGcC"}},{"cell_type":"code","source":"!if [ -d deepul ]; then rm -Rf deepul; fi\n!git clone https://github.com/rll/deepul.git \n!unzip -qq deepul/homeworks/hw1/data/hw1_data.zip -d deepul/homeworks/hw1/data/\n!pip install ./deepul","metadata":{"id":"wUVy2glDtoaR","execution":{"iopub.status.busy":"2023-04-16T22:22:34.474592Z","iopub.execute_input":"2023-04-16T22:22:34.475034Z","iopub.status.idle":"2023-04-16T22:23:01.714778Z","shell.execute_reply.started":"2023-04-16T22:22:34.474999Z","shell.execute_reply":"2023-04-16T22:23:01.713061Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from deepul.hw1_helper import *","metadata":{"id":"ZHWosWrbpO5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('kaggle test')","metadata":{"execution":{"iopub.status.busy":"2023-04-16T22:42:56.889556Z","iopub.execute_input":"2023-04-16T22:42:56.890843Z","iopub.status.idle":"2023-04-16T22:42:56.897576Z","shell.execute_reply.started":"2023-04-16T22:42:56.890779Z","shell.execute_reply":"2023-04-16T22:42:56.896091Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"kaggle test\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Question 1: 1D Data\n\nIn this question, we will train simple generative models on discrete 1D data.\n\nExecute the cell below to visualize our datasets","metadata":{"id":"7E4CMktzo100"}},{"cell_type":"code","source":"visualize_q1_data(dset_type=1)\nvisualize_q1_data(dset_type=2)","metadata":{"id":"ehhv2FZGo4_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (a) Fitting a Histogram\n\nLet $\\theta = (\\theta_0, \\dots, \\theta_{d-1}) \\in \\mathbb{R}^d$ and define the model $p_\\theta(x) = \\frac{e^{\\theta_x}}{\\sum_{x'}e^{\\theta_{x'}}}$\n\nFit $p_\\theta$ with maximum likelihood via stochastic gradient descent on the training set, using $\\theta$ initialized to zero. Use your favorite version of stochastic gradient descent, and optimize your hyperparameters on a validation set of your choice.\n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. Plot the model probabilities in a bar graph with $\\{0,\\dots,d-1\\}$ on the x-axis and a real number in $[0,1]$ on the y-axis.\n\n\n","metadata":{"id":"kSGTVznZqAR3"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"Yg0Jmo1PSaE4"}},{"cell_type":"code","source":"def q1_a(train_data, test_data, d, dset_id):\n  \"\"\"\n  train_data: An (n_train,) numpy array of integers in {0, ..., d-1}\n  test_data: An (n_test,) numpy array of integers in {0, .., d-1}\n  d: The number of possible discrete values for random variable x\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n             used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (d,) of model probabilities\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"BJNa6dHKpEQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q1_a`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"SiGBSP-ESeIj"}},{"cell_type":"code","source":"q1_save_results(1, 'a', q1_a)","metadata":{"id":"qjK_KReXsqYa","outputId":"fe6ae57d-63d8-4e50-a3ed-b0fb087a633f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1_save_results(2, 'a', q1_a)","metadata":{"id":"sJVOUEaaZXcA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (b) Fitting Discretized Mixture of Logistics\n\nLet us model $p_\\theta(x)$ as a **discretized** mixture of 4 logistics such that $p_\\theta(x) = \\sum_{i=1}^4 \\pi_i[\\sigma((x+0.5 - \\mu_i)/s_i) - \\sigma((x-0.5-\\mu_i)/s_i)]$\n\nFor the edge case of when $x = 0$, we replace $x-0.5$ by $-\\infty$, and for $x = d-1$, we replace $x+0.5$ by $\\infty$.\n\nYou may find the [PixelCNN++](https://arxiv.org/abs/1701.05517) helpful for more information on discretized mixture of logistics.\n\n**Provide the same set of corresponding deliverables as part (a)**","metadata":{"id":"DiyFXlj0rfcr"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"f4dnQIg_TDx6"}},{"cell_type":"code","source":"def q1_b(train_data, test_data, d, dset_id):\n  \"\"\"\n  train_data: An (n_train,) numpy array of integers in {0, ..., d-1}\n  test_data: An (n_test,) numpy array of integers in {0, .., d-1}\n  d: The number of possible discrete values for random variable x\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (d,) of model probabilities\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"uAvMQDJJrjNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q1_b`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"VwZyhlewTHH4"}},{"cell_type":"code","source":"q1_save_results(1, 'b', q1_b)","metadata":{"id":"wnnQORaG6Ouf","outputId":"3890f9ce-84cb-4c48-f2ca-9a1746acc424"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1_save_results(2, 'b', q1_b)","metadata":{"id":"1jLGoDa46RM6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 2: MADE\n\nIn this question, you will implement [MADE](https://arxiv.org/abs/1502.03509). In the first part, you will use MADE to model a simple 2D joint distribution, and in the second half, you will train MADE on image datasets.","metadata":{"id":"bk6l6G30tEIg"}},{"cell_type":"markdown","source":"## Part (a) Fitting 2D Data\n\nFirst, you will work with bivariate data of the form $x = (x_0,x_1)$, where $x_0, x_1 \\in \\{0, \\dots, d\\}$. We can easily visualize a 2D dataset by plotting a 2D histogram. Run the cell below to visualize our datasets.","metadata":{"id":"ZkQMNxln-UxX"}},{"cell_type":"code","source":"visualize_q2a_data(dset_type=1)\nvisualize_q2a_data(dset_type=2)","metadata":{"id":"alF9C1t-tEys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implement and train a MADE model through maximum likelihood to represent $p(x_0, x_1)$ on the given datasets, with any autoregressive ordering of your choosing. \n\nA few notes:\n* You do not need to do training with multiple masks\n* You made find it useful to one-hot encode your inputs. \n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. Visualize the learned 2D distribution by plotting a 2D heatmap\n","metadata":{"id":"sDywq5JZR4Eg"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"fHHxHqSJTSAa"}},{"cell_type":"code","source":"def q2_a(train_data, test_data, d, dset_id):\n  \"\"\"\n  train_data: An (n_train, 2) numpy array of integers in {0, ..., d-1}\n  test_data: An (n_test, 2) numpy array of integers in {0, .., d-1}\n  d: The number of possible discrete values for each random variable x1 and x2\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (d, d) of probabilities (the learned joint distribution)\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"wtFZU2ymB6_t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q2_a`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"Iso12bj4Tup8"}},{"cell_type":"code","source":"q2_save_results(1, 'a', q2_a)","metadata":{"id":"njfGrZ74Jm2d","outputId":"9edc164e-9469-462c-abd6-68f48f65f155"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q2_save_results(2, 'a', q2_a)","metadata":{"id":"o6-NJJnIJoX6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (b) Shapes and MNIST\nNow, we will work with a higher dimensional datasets, namely a shape dataset and MNIST. Run the cell below to visualize the two datasets","metadata":{"id":"G4HezJRtW7H2"}},{"cell_type":"code","source":"visualize_q2b_data(1)\nvisualize_q2b_data(2)","metadata":{"id":"F81U6yR1UUVq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implement and train a MADE model on the given binary image datasets. Given some binary image of height $H$ and width $W$, we can represent image $x\\in \\{0, 1\\}^{H\\times W}$ as a flattened binary vector $x\\in \\{0, 1\\}^{HW}$ to input into MADE to model $p_\\theta(x) = \\prod_{i=1}^{HW} p_\\theta(x_i|x_{<i})$. Your model should output logits, after which you could apply a sigmoid over 1 logit, or a softmax over two logits (either is fine).\n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. 100 samples from the final trained model","metadata":{"id":"qrJPvrYhUZYO"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"uEgF3FCXV8zb"}},{"cell_type":"code","source":"def q2_b(train_data, test_data, image_shape, dset_id):\n  \"\"\"\n  train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n  test_data: An (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n  image_shape: (H, W), height and width of the image\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"WHLjAxjRVx_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q2_b`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"iBfCSKFnV-Mz"}},{"cell_type":"code","source":"q2_save_results(1, 'b', q2_b)","metadata":{"id":"frAYhilEwG2x","outputId":"6f1db009-bf76-42f7-e9d8-af55239bc5b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q2_save_results(2, 'b', q2_b)","metadata":{"id":"B5nBFeI7wJeN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 3 PixelCNNs","metadata":{"id":"dP8lmmk7Xrct"}},{"cell_type":"markdown","source":"Now, you will train more powerful PixleCNN models on the shapes dataset and MNIST. In addition, we will extend to modelling colored datasets with and without channel conditioning.\n\n","metadata":{"id":"4wnyhDNqcAcw"}},{"cell_type":"markdown","source":"## Part (a) PixelCNN on Shapes and MNIST\nIn this part, implement a simple PixelCNN architecture to model binary MNIST and shapes images (same as Q2(b), but with a PixelCNN).\n\nWe recommend the following network design:\n* A $7 \\times 7$ masked type A convolution\n* $5$ $7 \\times 7$ masked type B convolutions\n* $2$ $1 \\times 1$ masked type B convolutions\n* Appropriate ReLU nonlinearities in-between\n* 64 convolutional filters\n\nAnd the following hyperparameters:\n* Batch size 128\n* Learning rate $10^{-3}$\n* 10 epochs\n* Adam Optimizer (this applies to all PixelCNN models trained in future parts)\n\nYour model should output logits, after which you could apply a sigmoid over 1 logit, or a softmax over two logits (either is fine). It may also help to scale your input to $[-1, 1]$ before running it through the network. \n\nTraining on the shapes dataset should be quick, and MNIST should take around 10 minutes\n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. 100 samples from the final trained model\n\n","metadata":{"id":"50WsEzhx4Uua"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"EleefdNuciyc"}},{"cell_type":"code","source":"def q3_a(train_data, test_data, image_shape, dset_id):\n  \"\"\"\n  train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n  test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n  image_shape: (H, W), height and width of the image\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"NWualafa-tpD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q3_a`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"v0EPVfz1cpq0"}},{"cell_type":"code","source":"q3a_save_results(1, q3_a)","metadata":{"id":"xNxXqVZpAd_V","outputId":"9280e0c4-d8de-408d-e0d6-af0ffa777527"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q3a_save_results(2, q3_a)","metadata":{"id":"OCyQzhJdAfiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (b) PixelCNN on Colored Shapes and MNIST: Independent Color Channels\n\nFor the next two parts, we'll work with color images (shapes and MNIST). Run the cell below to visualize the dataset.","metadata":{"id":"8J7qlqlODNgL"}},{"cell_type":"code","source":"visualize_q3b_data(1)\nvisualize_q3b_data(2)","metadata":{"id":"80f_7uZWkDSv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, implement a PixelCNN to support RGB color channels (or augment your existing implementation). **First, implement a PixelCNN that assumes color channels as independent.** More formally, we model the following parameterized distribution:\n\n$$p_\\theta(x) = \\prod_{i=1}^{HW}\\prod_{c=1}^C p_\\theta(x_i^c | x_{<i})$$\n\nHere are some tips that you may find useful for designing and training these models:\n* You will need a 4-way softmax for every prediction, as opposed to a 256-way softmax in the PixelCNN paper, since the dataset is quantized to two bits per color channel\n* You can set number of filters for each convolutions to 120. You can use the ReLU nonlinearity throughout.\n* Use a stack of 8 residual block architecture from [Figure 5](https://arxiv.org/abs/1601.06759) but with 7 x 7 masked convolutions in the middle instead of 3 x 3 masked convolutions\n* Consider using [layer normalization](https://arxiv.org/abs/1607.06450) to improve performance. However, be careful to maintain the autoregressive property.\n* With a learning rate of $10^{-3}$ and a batch size of 128, it should take a few minutes to run on the shapes dataset, and about 50-60 minutes on MNIST.\n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. 100 samples from the final trained model\n\n","metadata":{"id":"3Y6NggR6gmU9"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"GwJQG9i1iQOa"}},{"cell_type":"code","source":"def q3_b(train_data, test_data, image_shape, dset_id):\n  \"\"\"\n  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  image_shape: (H, W, C), height, width, and # of channels of the image\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, H, W, C) of samples with values in {0, 1, 2, 3}\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"NE99xTPJDLM7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q3_b`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"mGp2OsLKiToN"}},{"cell_type":"code","source":"q3bc_save_results(1, 'b', q3_b)","metadata":{"id":"kW-k-59qJaKN","outputId":"78b08f6a-12db-46b7-ac27-a07693a9cd09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q3bc_save_results(2, 'b', q3_b)","metadata":{"id":"It_iPXaZjlk0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (c) PixelCNN on Colored Shapes and MNIST: Autoregressive Color Channels\n\nNow, implement a PixelCNN that models **dependent** color channels. Formally, we model the parameterized distribution\n\n$$p_\\theta(x) = \\prod_{i=1}^{HW}\\prod_{c=1}^C p_\\theta(x_i^c | x_i^{<c}, x_{<i})$$\n\nTo do so, change your masking scheme for the center pixel. Split the filters into 3 groups, only allowing each group to see the groups before (or including the current group, for type B masks) to maintain the autoregressive property.\n\nTraining times and hyperparameter settings should be the same as part (b).\n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. 100 samples from the final trained model\n\n","metadata":{"id":"HlX23th4JbRy"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"_4VvvEJQK3Bb"}},{"cell_type":"code","source":"def q3_c(train_data, test_data, image_shape, dset_id):\n  \"\"\"\n  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  image_shape: (H, W, C), height, width, and # of channels of the image\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, H, W, C) of samples with values in {0, 1, 2, 3}\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"kqzuSDBUK3dL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q3_c`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"UTB-VUBnK321"}},{"cell_type":"code","source":"q3bc_save_results(1, 'c', q3_c)","metadata":{"id":"TXxpdINJK4NU","outputId":"904a8d40-d0e7-40bb-d13b-32c1a49298b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q3bc_save_results(2, 'c', q3_c)","metadata":{"id":"Q-Yh9D0G4PxQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (d) Conditional PixelCNNs\n\nIn this part, implement and train a **class-conditional** PixelCNN on binary MNIST. Condition on a class label by adding a conditional bias in each convolutional layer. More precisely, in the $\\ell$th convolutional layer, compute: \n$$W_\\ell * x + b_\\ell + V_\\ell y$$\nwhere $W_\\ell * x + b_\\ell$ is a masked convolution (as in previous parts), $V$ is a 2D weight matrix, and $y$ is a one-hot encoding of the class label (where the conditional bias is broadcasted spacially and added channel-wise).\n\nYou can use a PixelCNN architecture similar to part (a). Training on the shapes dataset should be quick, and MNIST should take around 10-15 minutes\n\n\n**You will provide these deliverables**\n\n\n1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n2.   Report the final test set performance of your final model\n3. 100 samples from the final trained model","metadata":{"id":"_IhTtaEXF633"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"8U3JwQV1izoA"}},{"cell_type":"code","source":"def q3_d(train_data, train_labels, test_data, test_labels, image_shape, n_classes, dset_id):\n  \"\"\"\n  train_data: A (n_train, H, W, 1) numpy array of binary images with values in {0, 1}\n  train_labels: A (n_train,) numpy array of class labels\n  test_data: A (n_test, H, W, 1) numpy array of binary images with values in {0, 1}\n  test_labels: A (n_test,) numpy array of class labels\n  image_shape: (H, W), height and width\n  n_classes: number of classes (4 or 10)\n  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n           used to set different hyperparameters for different datasets\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, H, C, 1) of samples with values in {0, 1}\n    where an even number of images of each class are sampled with 100 total\n  \"\"\"\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"En2kM8jhF8ri"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q3_d`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"LNSDCpfeiY0-"}},{"cell_type":"code","source":"q3d_save_results(1, q3_d)","metadata":{"id":"PhonsHCcGC3l","outputId":"a292b409-a20b-4056-c0c0-8d414d6230e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q3d_save_results(2, q3_d)","metadata":{"id":"ZtN6ykwn4MQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 4: Bonus Questions (Optional)","metadata":{"id":"LMZLcaHwLNNL"}},{"cell_type":"markdown","source":"## Part (a) Gated PixelCNN\nImplement a [Gated PixelCNN](https://arxiv.org/abs/1606.05328) to fix the blind-spot issue, and report training curves, final test loss, and samples.","metadata":{"id":"uZ-DhcsGLTRB"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"orP7ESaE5xI8"}},{"cell_type":"code","source":"def q4_a(train_data, test_data, image_shape):\n  \"\"\"\n  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  image_shape: (H, W, C), height, width, and # of channels of the image\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, H, W, C) of generated samples with values in {0, 1, 2, 3}\n  \"\"\"\n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"IlX6Aa6jLOGN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q4_a`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"AMdpZFJv55k9"}},{"cell_type":"code","source":"q4a_save_results(q4_a)","metadata":{"id":"zUjIxe2-8v-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (b) Grayscale PixelcNN\nTrain a [Grayscale PixelCNN](https://arxiv.org/abs/1612.08185) on Colored MNIST. You do not need to use their architecture - stacking standard masked convolutions or residual blocks is fine. First, generate a binary image, and then the 2-bit color image.","metadata":{"id":"dwV0spI2Ly0P"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"3DAJGnNv50C2"}},{"cell_type":"code","source":"def q4_b(train_data, test_data, image_shape):\n  \"\"\"\n  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n  image_shape: (H, W, C), height, width, and # of channels of the image\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (50, H, W, 1) of generated binary images in {0, 1}\n  - a numpy array of size (50, H, W, C) of conditonally generated color images in {0, 1, 2, 3}\n  \"\"\"\n  # You will need to generate the binary image dataset from train_data and test_data\n  \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"3Kix-StXL08n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q4_b`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"LPOmIqtM585K"}},{"cell_type":"code","source":"q4b_save_results(q4_b)","metadata":{"id":"iwjE85Ux83kv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part (c) Parallel Multiscale PixelCNN\nOne large disadvantage of autoregressive models is their slow sampling speed, since they require one network evaluation per feature. However, there are existing methods which introduce different independence assumptions to allow for parallelism when sampling. Implement a [Parallel PixelCNN](https://arxiv.org/abs/1703.03664) on 56 x 56 MNIST images, with a base size of 7 x 7 and upscaling by a factor of 2. Sampling should be very quick (< 1s). Architectures may vary, but using small PixelCNN implementation similar to previous parts and small ResNets should suffice","metadata":{"id":"7J9F2kFBL-p3"}},{"cell_type":"markdown","source":"### Solution\nFill out the function below and return the necessary arguments. Feel free to create more cells if need be.","metadata":{"id":"PfQAN_a051BS"}},{"cell_type":"code","source":"def q4_c(train_data, test_data):\n  \"\"\"\n  train_data: A (60000, 56, 56, 1) numpy array of grayscale images with values in {0, 1}\n  test_data: A (10000, 56, 56, 1) numpy array of grayscale images with values in {0, 1}\n  image_shape: (H, W), height and width\n\n  Returns\n  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n  - a numpy array of size (100, 56, 56, 1) of generated samples with values in {0, 1}\n  \"\"\"\n \n  \"\"\" YOUR CODE HERE \"\"\"","metadata":{"id":"Qa0Sm3GIMA5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n\nOnce you've implemented `q4_c`, execute the cells below to visualize and save your results\n\n","metadata":{"id":"2kFqvnZN517N"}},{"cell_type":"code","source":"q4c_save_results(q4_c)","metadata":{"id":"V73EuNZY5_Ld"},"execution_count":null,"outputs":[]}]}