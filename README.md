# deepul

My solutions to Berkeley's CS294 Deep Unsupervised Learning course.

## [HW 1 (2020): Autoregressive Models](https://github.com/parmarkrish/deepul/blob/main/hw1_autoregressive_models.ipynb)

- Q1: a) Fitting histogram model to 1D distribution of data, b) Fitting Discretized Mixture of Logistics
- Q2: Implementation of MADE (Masked Autoencoder for Distribution Estimation)
- Q3: Implementations of PixelCNNs with increasing level of sophistication

## [HW 1 (2024): Autoregressive Models](https://github.com/parmarkrish/deepul/blob/main/hw1_2024_autoregressive_models.ipynb)

- Q1, Q2: Skipped since redundent with 2020 edition
- Q3: iGPT implementation and transformer KV caching
- Q4: Transformer on VQ-VAE Image tokens
- Q5: Transformer on Text (with text-preprocessing code)
- Q6: Multimodal Transformer (able to generate text conditioned on images and generate images conditioned on text)

## [HW 2 (2020): Flow Models](https://github.com/parmarkrish/deepul/blob/main/hw2_flow_models.ipynb)

- Q1: Fitting 2D data with a) Autoregressive flow and b) RealNVP
- Q2: Implementation of Autoregressive Flow on images
- Q3: Implementation of RealNVP on images

## [HW 3 (2020): Latent Variable Models](https://github.com/parmarkrish/deepul/blob/main/hw3_latent_variable_models.ipynb)

- Q1: Fitting VAEs on 2D data
- Q2: Implementation of a) VAEs on images and b) VAEs with autoregressive flow prior
- Q3: VQ-VAE implementation

## [HW 3 (2024): GANs](https://github.com/parmarkrish/deepul/blob/main/hw3_2024_GANs.ipynb)

- Q1: Toy GAN on toy data (with saturating and non-saturating loss formulation)
- Q2: WGAN (with gradient penalty) on CIFAR-10
- Q3: VQGAN and VIT-VQGAN
